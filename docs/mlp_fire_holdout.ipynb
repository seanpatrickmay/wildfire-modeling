{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cc8048",
   "metadata": {},
   "source": [
    "# MLP Fire Holdout (By-Fire Split)\n",
    "\n",
    "This notebook trains a small MLP to predict whether the **center cell** will cross a GOES confidence threshold at time `t+1`, using the **3x3 neighborhood** features at time `t`.\n",
    "\n",
    "- **Input (X)**: 3x3 patch (center + 8 neighbors), 7 variables per cell (63 features total).\n",
    "- **Target (y)**: `1` iff `GOES_confidence_center(t+1) >= POSITIVE_THRESHOLD`, else `0`.\n",
    "- **Split**: train on all fires except `TEST_FIRES`; evaluate only on `TEST_FIRES`.\n",
    "\n",
    "Data layout expectation (same as `docs/neighbor_cell_confidence_regression.ipynb`):\n",
    "- `data/multi_fire/<FireName>/*GOES*json`\n",
    "- `data/multi_fire/<FireName>/rtma/rtma_manifest.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf3d040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/seanmay/Desktop/Current Projects/wildfire-prediction/docs\n",
      "fire selection: all\n",
      "test fires: ['Dixie', 'Kincade']\n",
      "positive confidence threshold: 0.1\n",
      "classification probability threshold: 0.5\n",
      "epochs: 1\n",
      "batch size: 8192\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# --- Split config ---\n",
    "FIRE_SELECTION = \"all\"  # \"all\" or list of fire names\n",
    "TEST_FIRES = [\"Dixie\", \"Kincade\"]\n",
    "\n",
    "# --- Task config ---\n",
    "POSITIVE_THRESHOLD = 0.10\n",
    "CLASSIFICATION_PROB_THRESHOLD = 0.50\n",
    "\n",
    "# --- Training config ---\n",
    "SEED = 1337\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 8192\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "HIDDEN_DIMS = [128, 64]\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Optional safety knobs (default: full data)\n",
    "MAX_HOURS_PER_FIRE = None        # e.g., 48\n",
    "MAX_SAMPLES_PER_HOUR = None      # e.g., 200_000\n",
    "\n",
    "print(\"cwd:\", Path.cwd())\n",
    "print(\"fire selection:\", FIRE_SELECTION)\n",
    "print(\"test fires:\", TEST_FIRES)\n",
    "print(\"positive confidence threshold:\", POSITIVE_THRESHOLD)\n",
    "print(\"classification probability threshold:\", CLASSIFICATION_PROB_THRESHOLD)\n",
    "print(\"epochs:\", EPOCHS)\n",
    "print(\"batch size:\", BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18335c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.5.1\n",
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import hashlib\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.warp import Resampling, reproject\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "def pick_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # Apple Silicon\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac48ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_iso(value: str) -> datetime:\n",
    "    if value.endswith(\"Z\"):\n",
    "        value = value[:-1] + \"+00:00\"\n",
    "    return datetime.fromisoformat(value)\n",
    "\n",
    "\n",
    "def normalize_time_str(value: str) -> str:\n",
    "    dt = parse_iso(value)\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:00:00Z\")\n",
    "\n",
    "\n",
    "def affine_from_list(vals: list) -> rasterio.Affine:\n",
    "    return rasterio.Affine(vals[0], vals[1], vals[2], vals[3], vals[4], vals[5])\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"data\").exists() and (p / \"scripts\").exists() and (p / \"docs\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not find repo root containing data/, scripts/, docs/.\")\n",
    "\n",
    "\n",
    "def load_goes_times(goes_meta: dict, goes_conf: np.ndarray):\n",
    "    goes_time_steps = goes_meta.get(\"time_steps\", [])\n",
    "    goes_start = goes_meta.get(\"start_time\")\n",
    "\n",
    "    if goes_time_steps and isinstance(goes_time_steps[0], (int, float)):\n",
    "        if not goes_start:\n",
    "            raise ValueError(\"GOES time_steps are numeric and metadata.start_time is missing.\")\n",
    "        start_dt = parse_iso(goes_start)\n",
    "        goes_time_steps = [\n",
    "            (start_dt + timedelta(hours=int(i - 1))).strftime(\"%Y-%m-%dT%H:00:00Z\")\n",
    "            for i in goes_time_steps\n",
    "        ]\n",
    "    elif not goes_time_steps and goes_start:\n",
    "        start_dt = parse_iso(goes_start)\n",
    "        goes_time_steps = [\n",
    "            (start_dt + timedelta(hours=i)).strftime(\"%Y-%m-%dT%H:00:00Z\")\n",
    "            for i in range(goes_conf.shape[0])\n",
    "        ]\n",
    "    else:\n",
    "        goes_time_steps = [normalize_time_str(t) for t in goes_time_steps]\n",
    "\n",
    "    if not goes_time_steps:\n",
    "        raise ValueError(\"GOES metadata has no usable time_steps.\")\n",
    "\n",
    "    return goes_time_steps\n",
    "\n",
    "\n",
    "def discover_fire_entries(repo_root: Path):\n",
    "    base = repo_root / \"data\" / \"multi_fire\"\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"Missing multi-fire directory: {base}\")\n",
    "\n",
    "    entries = []\n",
    "    for fire_dir in sorted([d for d in base.iterdir() if d.is_dir()]):\n",
    "        goes_candidates = sorted(fire_dir.glob(\"*GOES*json\"))\n",
    "        manifest_path = fire_dir / \"rtma\" / \"rtma_manifest.json\"\n",
    "        if not goes_candidates or not manifest_path.exists():\n",
    "            continue\n",
    "        entries.append(\n",
    "            {\n",
    "                \"fire_name\": fire_dir.name,\n",
    "                \"goes_json\": goes_candidates[0],\n",
    "                \"rtma_manifest\": manifest_path,\n",
    "            }\n",
    "        )\n",
    "    return entries\n",
    "\n",
    "\n",
    "def select_fire_entries(entries, fire_selection):\n",
    "    if fire_selection is None or fire_selection == \"all\":\n",
    "        return entries\n",
    "\n",
    "    if not isinstance(fire_selection, (list, tuple, set)):\n",
    "        raise ValueError('FIRE_SELECTION must be \"all\" or a list/tuple/set of fire names.')\n",
    "\n",
    "    wanted = {str(x) for x in fire_selection}\n",
    "    selected = [e for e in entries if e[\"fire_name\"] in wanted]\n",
    "    found = {e[\"fire_name\"] for e in selected}\n",
    "    missing = sorted(wanted - found)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Unknown fire names in FIRE_SELECTION: {missing}\")\n",
    "    return selected\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfdd6163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature count: 63\n",
      "first 10 feature names: ['GOES_conf_c', 'TMP_c', 'WIND_c', 'SPFH_c', 'ACPC01_c', 'WDIR_sin_c', 'WDIR_cos_c', 'GOES_conf_nw', 'TMP_nw', 'WIND_nw']\n"
     ]
    }
   ],
   "source": [
    "CELL_OFFSETS = [\n",
    "    (\"c\", 0, 0),\n",
    "    (\"nw\", -1, -1),\n",
    "    (\"n\", -1, 0),\n",
    "    (\"ne\", -1, 1),\n",
    "    (\"w\", 0, -1),\n",
    "    (\"e\", 0, 1),\n",
    "    (\"sw\", 1, -1),\n",
    "    (\"s\", 1, 0),\n",
    "    (\"se\", 1, 1),\n",
    "]\n",
    "\n",
    "VAR_ORDER = [\"GOES_conf\", \"TMP\", \"WIND\", \"SPFH\", \"ACPC01\", \"WDIR_sin\", \"WDIR_cos\"]\n",
    "RTMA_VARS_REQUIRED = [\"TMP\", \"WIND\", \"WDIR\", \"SPFH\", \"ACPC01\"]\n",
    "\n",
    "\n",
    "def feature_names():\n",
    "    names = []\n",
    "    for n_name, _, _ in CELL_OFFSETS:\n",
    "        for v in VAR_ORDER:\n",
    "            names.append(f\"{v}_{n_name}\")\n",
    "    return names\n",
    "\n",
    "\n",
    "FEATURE_NAMES = feature_names()\n",
    "N_FEATURES = len(FEATURE_NAMES)\n",
    "\n",
    "\n",
    "def to_binary_target(y_continuous: np.ndarray, threshold: float) -> np.ndarray:\n",
    "    return (y_continuous >= threshold).astype(np.int32)\n",
    "\n",
    "\n",
    "def resolve_manifest_file_path(path_str: str, repo_root: Path, manifest_dir: Path) -> Path:\n",
    "    p = Path(path_str).expanduser()\n",
    "    if p.exists():\n",
    "        return p\n",
    "\n",
    "    parts = p.parts\n",
    "    if \"data\" in parts:\n",
    "        idx = parts.index(\"data\")\n",
    "        candidate = repo_root.joinpath(*parts[idx:])\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "\n",
    "    candidate = (manifest_dir / path_str).resolve()\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not resolve RTMA part path: {path_str}\")\n",
    "\n",
    "\n",
    "def resample_stack(src_stack, src_transform, src_crs, dst_shape, dst_transform, dst_crs):\n",
    "    bands = src_stack.shape[0]\n",
    "    dst = np.empty((bands, dst_shape[0], dst_shape[1]), dtype=np.float32)\n",
    "    for b in range(bands):\n",
    "        reproject(\n",
    "            source=src_stack[b],\n",
    "            destination=dst[b],\n",
    "            src_transform=src_transform,\n",
    "            src_crs=src_crs,\n",
    "            dst_transform=dst_transform,\n",
    "            dst_crs=dst_crs,\n",
    "            resampling=Resampling.bilinear,\n",
    "        )\n",
    "    return dst\n",
    "\n",
    "\n",
    "def build_hour_samples(conf_t, conf_t1, rtma_hour):\n",
    "    # Use only interior cells so center + all 8 neighbors exist.\n",
    "    h, w = conf_t.shape\n",
    "    if h < 3 or w < 3:\n",
    "        return np.empty((0, N_FEATURES), dtype=np.float64), np.empty((0,), dtype=np.float64)\n",
    "\n",
    "    y = conf_t1[1:-1, 1:-1].astype(np.float64)\n",
    "    feat_blocks = []\n",
    "\n",
    "    for _, dy, dx in CELL_OFFSETS:\n",
    "        ys = slice(1 + dy, h - 1 + dy)\n",
    "        xs = slice(1 + dx, w - 1 + dx)\n",
    "\n",
    "        go_cell = conf_t[ys, xs].astype(np.float64)\n",
    "        tmp_cell = rtma_hour[\"TMP\"][ys, xs].astype(np.float64)\n",
    "        wind_cell = rtma_hour[\"WIND\"][ys, xs].astype(np.float64)\n",
    "        spfh_cell = rtma_hour[\"SPFH\"][ys, xs].astype(np.float64)\n",
    "        precip_cell = rtma_hour[\"ACPC01\"][ys, xs].astype(np.float64)\n",
    "        wdir_deg_cell = rtma_hour[\"WDIR\"][ys, xs].astype(np.float64)\n",
    "        wdir_rad_cell = np.deg2rad(wdir_deg_cell)\n",
    "        wdir_sin_cell = np.sin(wdir_rad_cell)\n",
    "        wdir_cos_cell = np.cos(wdir_rad_cell)\n",
    "\n",
    "        feat_blocks.extend([\n",
    "            go_cell,\n",
    "            tmp_cell,\n",
    "            wind_cell,\n",
    "            spfh_cell,\n",
    "            precip_cell,\n",
    "            wdir_sin_cell,\n",
    "            wdir_cos_cell,\n",
    "        ])\n",
    "\n",
    "    X = np.stack(feat_blocks, axis=-1).reshape(-1, N_FEATURES)\n",
    "    y = y.reshape(-1)\n",
    "\n",
    "    valid = np.isfinite(y)\n",
    "    valid &= np.isfinite(X).all(axis=1)\n",
    "\n",
    "    if not valid.any():\n",
    "        return np.empty((0, N_FEATURES), dtype=np.float64), np.empty((0,), dtype=np.float64)\n",
    "\n",
    "    X = X[valid]\n",
    "    y = y[valid]\n",
    "\n",
    "    if MAX_SAMPLES_PER_HOUR is not None and X.shape[0] > MAX_SAMPLES_PER_HOUR:\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        idx = rng.choice(X.shape[0], size=int(MAX_SAMPLES_PER_HOUR), replace=False)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def iter_aligned_hours_for_fire(\n",
    "    goes_conf,\n",
    "    goes_time_index,\n",
    "    rtma_manifest,\n",
    "    rtma_manifest_path: Path,\n",
    "    goes_shape,\n",
    "    goes_transform,\n",
    "    goes_crs,\n",
    "):\n",
    "    rtma_vars = rtma_manifest[\"variables\"]\n",
    "    for req in RTMA_VARS_REQUIRED:\n",
    "        if req not in rtma_vars:\n",
    "            raise KeyError(f\"RTMA manifest missing required variable: {req}\")\n",
    "\n",
    "    manifest_dir = rtma_manifest_path.parent\n",
    "    rtma_files = rtma_manifest[\"files\"]\n",
    "    resolved_files = {\n",
    "        var: [resolve_manifest_file_path(path, REPO_ROOT, manifest_dir) for path in rtma_files[var]]\n",
    "        for var in rtma_vars\n",
    "    }\n",
    "\n",
    "    n_parts = len(resolved_files[rtma_vars[0]])\n",
    "    for v in rtma_vars:\n",
    "        if len(resolved_files[v]) != n_parts:\n",
    "            raise ValueError(\"RTMA variable file lists do not have equal part counts.\")\n",
    "\n",
    "    parts = list(zip(*[resolved_files[v] for v in rtma_vars]))\n",
    "    rtma_time_steps = [normalize_time_str(t) for t in rtma_manifest[\"time_steps\"]]\n",
    "\n",
    "    rtma_time_ptr = 0\n",
    "\n",
    "    for part_paths in parts:\n",
    "        rtma_arrays = {}\n",
    "        rtma_transform = None\n",
    "        rtma_crs = None\n",
    "        band_count = None\n",
    "\n",
    "        for var, part_path in zip(rtma_vars, part_paths):\n",
    "            with rasterio.open(part_path) as ds:\n",
    "                if rtma_transform is None:\n",
    "                    rtma_transform = ds.transform\n",
    "                    rtma_crs = ds.crs\n",
    "                    band_count = ds.count\n",
    "                rtma_arrays[var] = ds.read().astype(\"float32\")\n",
    "\n",
    "        if band_count is None:\n",
    "            continue\n",
    "\n",
    "        resampled = {}\n",
    "        for var in rtma_vars:\n",
    "            resampled[var] = resample_stack(\n",
    "                rtma_arrays[var],\n",
    "                rtma_transform,\n",
    "                rtma_crs,\n",
    "                goes_shape,\n",
    "                goes_transform,\n",
    "                goes_crs,\n",
    "            )\n",
    "\n",
    "        for local_idx in range(band_count):\n",
    "            global_idx = rtma_time_ptr + local_idx\n",
    "            if global_idx >= len(rtma_time_steps):\n",
    "                break\n",
    "\n",
    "            time_str = rtma_time_steps[global_idx]\n",
    "            if time_str not in goes_time_index:\n",
    "                continue\n",
    "\n",
    "            t = goes_time_index[time_str]\n",
    "            if t + 1 >= goes_conf.shape[0]:\n",
    "                continue\n",
    "\n",
    "            rtma_hour = {var: resampled[var][local_idx] for var in RTMA_VARS_REQUIRED}\n",
    "            yield t, rtma_hour\n",
    "\n",
    "        rtma_time_ptr += band_count\n",
    "\n",
    "\n",
    "def iter_fire_hour_samples(entry):\n",
    "    with Path(entry[\"goes_json\"]).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        goes_json_local = json.load(f)\n",
    "    with Path(entry[\"rtma_manifest\"]).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        rtma_manifest_local = json.load(f)\n",
    "\n",
    "    goes_conf_local = np.array(goes_json_local[\"data\"], dtype=np.float32)\n",
    "    goes_meta_local = goes_json_local[\"metadata\"]\n",
    "    goes_transform_local = affine_from_list(goes_meta_local[\"geo_transform\"])\n",
    "    goes_crs_local = goes_meta_local.get(\"crs\")\n",
    "    goes_shape_local = tuple(goes_meta_local[\"grid_shape\"])\n",
    "    goes_times_local = load_goes_times(goes_meta_local, goes_conf_local)\n",
    "    goes_time_index_local = {t: i for i, t in enumerate(goes_times_local)}\n",
    "\n",
    "    hours_yielded = 0\n",
    "    for t, rtma_hour in iter_aligned_hours_for_fire(\n",
    "        goes_conf_local,\n",
    "        goes_time_index_local,\n",
    "        rtma_manifest_local,\n",
    "        Path(entry[\"rtma_manifest\"]),\n",
    "        goes_shape_local,\n",
    "        goes_transform_local,\n",
    "        goes_crs_local,\n",
    "    ):\n",
    "        X_hour, y_hour_cont = build_hour_samples(goes_conf_local[t], goes_conf_local[t + 1], rtma_hour)\n",
    "        if X_hour.shape[0] == 0:\n",
    "            continue\n",
    "        y_hour = to_binary_target(y_hour_cont, POSITIVE_THRESHOLD)\n",
    "        yield entry[\"fire_name\"], t, X_hour, y_hour\n",
    "\n",
    "        hours_yielded += 1\n",
    "        if MAX_HOURS_PER_FIRE is not None and hours_yielded >= MAX_HOURS_PER_FIRE:\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"feature count:\", N_FEATURES)\n",
    "print(\"first 10 feature names:\", FEATURE_NAMES[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4826ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available fires: ['August_Complex', 'Bobcat', 'CZU_Lightning_Complex', 'Creek', 'Dixie', 'Dolan', 'Glass', 'July_Complex', 'Kincade', 'LNU_Lightning_Complex', 'North_Complex', 'Red_Salmon_Complex', 'SCU_Lightning_Complex', 'SQF_Complex', 'Slater_and_Devil', 'W-5_Cold_Springs', 'Walker', 'Zogg']\n",
      "selected fires: ['August_Complex', 'Bobcat', 'CZU_Lightning_Complex', 'Creek', 'Dixie', 'Dolan', 'Glass', 'July_Complex', 'Kincade', 'LNU_Lightning_Complex', 'North_Complex', 'Red_Salmon_Complex', 'SCU_Lightning_Complex', 'SQF_Complex', 'Slater_and_Devil', 'W-5_Cold_Springs', 'Walker', 'Zogg']\n",
      "train fires: ['August_Complex', 'Bobcat', 'CZU_Lightning_Complex', 'Creek', 'Dolan', 'Glass', 'July_Complex', 'LNU_Lightning_Complex', 'North_Complex', 'Red_Salmon_Complex', 'SCU_Lightning_Complex', 'SQF_Complex', 'Slater_and_Devil', 'W-5_Cold_Springs', 'Walker', 'Zogg']\n",
      "test fires: ['Dixie', 'Kincade']\n"
     ]
    }
   ],
   "source": [
    "all_fire_entries = discover_fire_entries(REPO_ROOT)\n",
    "fire_entries = select_fire_entries(all_fire_entries, FIRE_SELECTION)\n",
    "\n",
    "available = [e[\"fire_name\"] for e in all_fire_entries]\n",
    "selected = [e[\"fire_name\"] for e in fire_entries]\n",
    "\n",
    "print(\"available fires:\", available)\n",
    "print(\"selected fires:\", selected)\n",
    "\n",
    "missing_test = sorted(set(TEST_FIRES) - set(available))\n",
    "if missing_test:\n",
    "    raise RuntimeError(f\"Missing TEST_FIRES in data/multi_fire: {missing_test}\")\n",
    "\n",
    "train_entries = [e for e in fire_entries if e[\"fire_name\"] not in set(TEST_FIRES)]\n",
    "test_entries = [e for e in fire_entries if e[\"fire_name\"] in set(TEST_FIRES)]\n",
    "\n",
    "print(\"train fires:\", [e[\"fire_name\"] for e in train_entries])\n",
    "print(\"test fires:\", [e[\"fire_name\"] for e in test_entries])\n",
    "\n",
    "if not train_entries:\n",
    "    raise RuntimeError(\"No training fires after split.\")\n",
    "if not test_entries:\n",
    "    raise RuntimeError(\"No test fires after split.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74db32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training hours used: 11329\n",
      "training samples: 78365262\n",
      "training positives: 222000\n",
      "training negatives: 78143262\n",
      "training pos rate: 0.0028328878680964533\n",
      "pos_weight (neg/pos): 351.9966756756757\n"
     ]
    }
   ],
   "source": [
    "def welford_update(count: int, mean: np.ndarray, m2: np.ndarray, X: np.ndarray):\n",
    "    # X: (n, d)\n",
    "    X = X.astype(np.float64, copy=False)\n",
    "    n = X.shape[0]\n",
    "    if n == 0:\n",
    "        return count, mean, m2\n",
    "\n",
    "    batch_mean = X.mean(axis=0)\n",
    "    batch_m2 = ((X - batch_mean) ** 2).sum(axis=0)\n",
    "\n",
    "    if count == 0:\n",
    "        return n, batch_mean, batch_m2\n",
    "\n",
    "    delta = batch_mean - mean\n",
    "    total = count + n\n",
    "    new_mean = mean + delta * (n / total)\n",
    "    new_m2 = m2 + batch_m2 + (delta ** 2) * (count * n / total)\n",
    "    return total, new_mean, new_m2\n",
    "\n",
    "\n",
    "# Pass A: compute normalization stats + class balance on TRAIN fires only.\n",
    "count = 0\n",
    "mean = np.zeros((N_FEATURES,), dtype=np.float64)\n",
    "m2 = np.zeros((N_FEATURES,), dtype=np.float64)\n",
    "\n",
    "train_pos = 0\n",
    "train_neg = 0\n",
    "train_samples = 0\n",
    "hours_used = 0\n",
    "\n",
    "for entry in train_entries:\n",
    "    for fire_name, t, X_hour, y_hour in iter_fire_hour_samples(entry):\n",
    "        hours_used += 1\n",
    "        train_samples += int(y_hour.shape[0])\n",
    "        train_pos += int(y_hour.sum())\n",
    "        train_neg += int(y_hour.shape[0] - y_hour.sum())\n",
    "\n",
    "        count, mean, m2 = welford_update(count, mean, m2, X_hour)\n",
    "\n",
    "if count < 2:\n",
    "    raise RuntimeError(\"Not enough training samples to compute normalization stats.\")\n",
    "\n",
    "var = m2 / (count - 1)\n",
    "std = np.sqrt(np.maximum(var, 1e-12))\n",
    "\n",
    "pos_weight_value = (train_neg / max(train_pos, 1))\n",
    "\n",
    "print(\"training hours used:\", hours_used)\n",
    "print(\"training samples:\", train_samples)\n",
    "print(\"training positives:\", train_pos)\n",
    "print(\"training negatives:\", train_neg)\n",
    "print(\"training pos rate:\", train_pos / max(train_samples, 1))\n",
    "print(\"pos_weight (neg/pos):\", pos_weight_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0774ec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=63, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dims: list[int], dropout: float):\n",
    "        super().__init__()\n",
    "        layers: list[nn.Module] = []\n",
    "        prev = in_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout and dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "model = MLP(N_FEATURES, HIDDEN_DIMS, DROPOUT).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32, device=DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9847ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1 avg_loss=0.352312 acc=0.9802 tpr=0.8764 tnr=0.9805 TP=194564 FP=1520461 FN=27436 TN=76622801\n"
     ]
    }
   ],
   "source": [
    "def batch_iter(n_rows: int, batch_size: int):\n",
    "    for start in range(0, n_rows, batch_size):\n",
    "        end = min(n_rows, start + batch_size)\n",
    "        yield start, end\n",
    "\n",
    "\n",
    "def logits_to_pred(logits: torch.Tensor, prob_threshold: float) -> torch.Tensor:\n",
    "    probs = torch.sigmoid(logits)\n",
    "    return (probs >= prob_threshold).to(torch.int32)\n",
    "\n",
    "\n",
    "# Train (streaming over hours)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_batches = 0\n",
    "\n",
    "    tp = fp = fn = tn = 0\n",
    "\n",
    "    for entry in train_entries:\n",
    "        for fire_name, t, X_hour, y_hour in iter_fire_hour_samples(entry):\n",
    "            Xn = (X_hour - mean) / std\n",
    "\n",
    "            y_np = y_hour.astype(np.float32, copy=False)\n",
    "            n_rows = Xn.shape[0]\n",
    "\n",
    "            for start, end in batch_iter(n_rows, BATCH_SIZE):\n",
    "                xb = torch.from_numpy(Xn[start:end]).to(device=DEVICE, dtype=torch.float32)\n",
    "                yb = torch.from_numpy(y_np[start:end]).to(device=DEVICE, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += float(loss.detach().cpu())\n",
    "                epoch_batches += 1\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred = logits_to_pred(logits, CLASSIFICATION_PROB_THRESHOLD)\n",
    "                    y_int = yb.to(torch.int32)\n",
    "                    tp += int(((pred == 1) & (y_int == 1)).sum().cpu())\n",
    "                    fp += int(((pred == 1) & (y_int == 0)).sum().cpu())\n",
    "                    fn += int(((pred == 0) & (y_int == 1)).sum().cpu())\n",
    "                    tn += int(((pred == 0) & (y_int == 0)).sum().cpu())\n",
    "\n",
    "    avg_loss = epoch_loss / max(epoch_batches, 1)\n",
    "    acc = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "    tpr = tp / max(tp + fn, 1)\n",
    "    tnr = tn / max(tn + fp, 1)\n",
    "\n",
    "    print(f\"epoch {epoch}/{EPOCHS} avg_loss={avg_loss:.6f} acc={acc:.4f} tpr={tpr:.4f} tnr={tnr:.4f} TP={tp} FP={fp} FN={fn} TN={tn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40704cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall test metrics: {'n': 25817908, 'accuracy': 0.9953858383878353, 'tpr': 0.8702959528307287, 'tnr': 0.9958104686632779, 'precision': 0.4135462285450045, 'tp': 76016, 'fp': 107799, 'fn': 11329, 'tn': 25622764}\n",
      "per-fire test metrics:\n",
      "Dixie {'n': 25370800, 'accuracy': 0.9954943478329418, 'tpr': 0.8696321926248933, 'tnr': 0.995914405873701, 'precision': 0.41533672891907186, 'tp': 73390, 'fp': 103310, 'fn': 11002, 'tn': 25183098, 'pos_rate': 0.003326343670676526}\n",
      "Kincade {'n': 447108, 'accuracy': 0.9892285532801918, 'tpr': 0.889265154080596, 'tnr': 0.9898931679256116, 'precision': 0.3690794096978215, 'tp': 2626, 'fp': 4489, 'fn': 327, 'tn': 439666, 'pos_rate': 0.006604668223337538}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on held-out fires (no gradient)\n",
    "model.eval()\n",
    "\n",
    "overall = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "per_fire = {}\n",
    "\n",
    "# Precision/Recall/F1 sweep (match logistic notebook style)\n",
    "PR_THRESHOLDS = np.linspace(0.0, 1.0, 1001)\n",
    "pr_tp = np.zeros(PR_THRESHOLDS.shape[0], dtype=np.int64)\n",
    "pr_fp = np.zeros(PR_THRESHOLDS.shape[0], dtype=np.int64)\n",
    "pr_total_pos = 0\n",
    "pr_total_neg = 0\n",
    "\n",
    "\n",
    "def batch_iter(n_rows: int, batch_size: int):\n",
    "    for start in range(0, n_rows, batch_size):\n",
    "        end = min(n_rows, start + batch_size)\n",
    "        yield start, end\n",
    "\n",
    "\n",
    "def metrics_from_counts(tp, fp, fn, tn):\n",
    "    n = tp + fp + fn + tn\n",
    "    acc = (tp + tn) / max(n, 1)\n",
    "    recall = tp / max(tp + fn, 1)\n",
    "    tpr = recall\n",
    "    tnr = tn / max(tn + fp, 1)\n",
    "    precision = tp / max(tp + fp, 1)\n",
    "    denom = (precision + recall)\n",
    "    f1 = (2 * precision * recall / denom) if denom > 0 else 0.0\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"tpr\": tpr,\n",
    "        \"tnr\": tnr,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"tn\": tn,\n",
    "    }\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for entry in test_entries:\n",
    "        fire = entry[\"fire_name\"]\n",
    "        per_fire.setdefault(fire, {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0, \"n\": 0, \"pos\": 0, \"neg\": 0})\n",
    "\n",
    "        for fire_name, t, X_hour, y_hour in iter_fire_hour_samples(entry):\n",
    "            Xn = (X_hour - mean) / std\n",
    "            y_np = y_hour.astype(np.int32, copy=False)\n",
    "\n",
    "            n_rows = Xn.shape[0]\n",
    "            for start, end in batch_iter(n_rows, BATCH_SIZE):\n",
    "                xb = torch.from_numpy(Xn[start:end]).to(device=DEVICE, dtype=torch.float32)\n",
    "                logits = model(xb)\n",
    "                probs = torch.sigmoid(logits).squeeze(1).detach().cpu().numpy()\n",
    "\n",
    "                yb = y_np[start:end]\n",
    "                pred = (probs >= CLASSIFICATION_PROB_THRESHOLD).astype(np.int32)\n",
    "\n",
    "                tp = int(((pred == 1) & (yb == 1)).sum())\n",
    "                fp = int(((pred == 1) & (yb == 0)).sum())\n",
    "                fn = int(((pred == 0) & (yb == 1)).sum())\n",
    "                tn = int(((pred == 0) & (yb == 0)).sum())\n",
    "\n",
    "                overall[\"tp\"] += tp\n",
    "                overall[\"fp\"] += fp\n",
    "                overall[\"fn\"] += fn\n",
    "                overall[\"tn\"] += tn\n",
    "\n",
    "                pf = per_fire[fire]\n",
    "                pf[\"tp\"] += tp\n",
    "                pf[\"fp\"] += fp\n",
    "                pf[\"fn\"] += fn\n",
    "                pf[\"tn\"] += tn\n",
    "                pf[\"n\"] += int(yb.shape[0])\n",
    "                pf[\"pos\"] += int(yb.sum())\n",
    "                pf[\"neg\"] += int(yb.shape[0] - yb.sum())\n",
    "\n",
    "                # Threshold sweep counts (exact for this threshold grid)\n",
    "                pos = yb == 1\n",
    "                pr_total_pos += int(pos.sum())\n",
    "                pr_total_neg += int((~pos).sum())\n",
    "\n",
    "                sweep_pred = probs[:, None] >= PR_THRESHOLDS[None, :]\n",
    "                pr_tp += (sweep_pred & pos[:, None]).sum(axis=0).astype(np.int64)\n",
    "                pr_fp += (sweep_pred & (~pos)[:, None]).sum(axis=0).astype(np.int64)\n",
    "\n",
    "\n",
    "overall_metrics = metrics_from_counts(**overall)\n",
    "print(\"overall test metrics:\", overall_metrics)\n",
    "\n",
    "per_fire_metrics = {}\n",
    "for fire, c in per_fire.items():\n",
    "    per_fire_metrics[fire] = metrics_from_counts(c[\"tp\"], c[\"fp\"], c[\"fn\"], c[\"tn\"])\n",
    "    per_fire_metrics[fire][\"pos_rate\"] = c[\"pos\"] / max(c[\"n\"], 1)\n",
    "\n",
    "print(\"per-fire test metrics:\")\n",
    "for fire in sorted(per_fire_metrics):\n",
    "    print(fire, per_fire_metrics[fire])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6934e5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pr_total_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Precision-Recall curve + top thresholds by F1 (Test-Fire Set)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pr_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpr_total_pos\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo positive samples in the held-out test set; cannot compute precision/recall.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m pr_fn \u001b[38;5;241m=\u001b[39m pr_total_pos \u001b[38;5;241m-\u001b[39m pr_tp\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pr_total_pos' is not defined"
     ]
    }
   ],
   "source": [
    "# Precision-Recall curve + top thresholds by F1 (Test-Fire Set)\n",
    "\n",
    "pr_df = None\n",
    "\n",
    "if pr_total_pos == 0:\n",
    "    raise RuntimeError(\"No positive samples in the held-out test set; cannot compute precision/recall.\")\n",
    "\n",
    "pr_fn = pr_total_pos - pr_tp\n",
    "pr_tn = pr_total_neg - pr_fp\n",
    "\n",
    "pr_precision = np.where((pr_tp + pr_fp) > 0, pr_tp / (pr_tp + pr_fp), 1.0)\n",
    "pr_recall = pr_tp / pr_total_pos\n",
    "\n",
    "pr_f1 = np.where(\n",
    "    (pr_precision + pr_recall) > 0,\n",
    "    2 * pr_precision * pr_recall / (pr_precision + pr_recall),\n",
    "    0.0,\n",
    ")\n",
    "\n",
    "pr_df = pd.DataFrame(\n",
    "    {\n",
    "        \"threshold\": PR_THRESHOLDS,\n",
    "        \"precision\": pr_precision,\n",
    "        \"recall\": pr_recall,\n",
    "        \"f1\": pr_f1,\n",
    "        \"tp\": pr_tp,\n",
    "        \"fp\": pr_fp,\n",
    "        \"fn\": pr_fn,\n",
    "        \"tn\": pr_tn,\n",
    "    }\n",
    ")\n",
    "\n",
    "best = pr_df.iloc[int(pr_df[\"f1\"].idxmax())]\n",
    "baseline = pr_total_pos / (pr_total_pos + pr_total_neg)\n",
    "\n",
    "print(\"test-fire positive rate (baseline precision at recall=1):\", float(baseline))\n",
    "print(\"best threshold by F1:\", float(best[\"threshold\"]))\n",
    "print(\"precision:\", float(best[\"precision\"]), \"recall:\", float(best[\"recall\"]), \"f1:\", float(best[\"f1\"]))\n",
    "\n",
    "pr_plot_df = pr_df.sort_values(\"recall\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(pr_plot_df[\"recall\"], pr_plot_df[\"precision\"], linewidth=2)\n",
    "plt.hlines(baseline, 0, 1, linestyles=\"dashed\", colors=\"gray\", label=f\"baseline (pos rate={baseline:.3f})\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Recall (TP / (TP+FN))\")\n",
    "plt.ylabel(\"Precision (TP / (TP+FP))\")\n",
    "plt.title(\"Precision-Recall Curve (Test-Fire Set)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "pr_df.sort_values(\"f1\", ascending=False).head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save report\n",
    "out_dir = REPO_ROOT / \"data\" / \"analysis\" / \"mlp_fire_holdout\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Approx AP via trapezoid on the PR curve (recall-sorted)\n",
    "pr_plot_df = pr_df.sort_values(\"recall\")\n",
    "trapz = getattr(np, \"trapezoid\", None)\n",
    "if trapz is None:\n",
    "    trapz = np.trapz\n",
    "approx_ap = float(trapz(pr_plot_df[\"precision\"].to_numpy(), pr_plot_df[\"recall\"].to_numpy()))\n",
    "\n",
    "best = pr_df.iloc[int(pr_df[\"f1\"].idxmax())]\n",
    "\n",
    "report = {\n",
    "    \"model\": \"mlp_pytorch\",\n",
    "    \"config\": {\n",
    "        \"fire_selection\": FIRE_SELECTION,\n",
    "        \"test_fires\": TEST_FIRES,\n",
    "        \"positive_threshold\": POSITIVE_THRESHOLD,\n",
    "        \"classification_prob_threshold\": CLASSIFICATION_PROB_THRESHOLD,\n",
    "        \"seed\": SEED,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"hidden_dims\": HIDDEN_DIMS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"max_hours_per_fire\": MAX_HOURS_PER_FIRE,\n",
    "        \"max_samples_per_hour\": MAX_SAMPLES_PER_HOUR,\n",
    "        \"n_features\": N_FEATURES,\n",
    "        \"feature_names\": FEATURE_NAMES,\n",
    "        \"pr_threshold_count\": int(PR_THRESHOLDS.shape[0]),\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"samples\": int(train_samples),\n",
    "        \"positives\": int(train_pos),\n",
    "        \"negatives\": int(train_neg),\n",
    "        \"pos_rate\": float(train_pos / max(train_samples, 1)),\n",
    "        \"pos_weight\": float(pos_weight_value),\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"overall\": overall_metrics,\n",
    "        \"per_fire\": per_fire_metrics,\n",
    "        \"threshold_sweep\": {\n",
    "            \"baseline_pos_rate\": float(pr_total_pos / (pr_total_pos + pr_total_neg)),\n",
    "            \"best_threshold_by_f1\": float(best[\"threshold\"]),\n",
    "            \"best_precision\": float(best[\"precision\"]),\n",
    "            \"best_recall\": float(best[\"recall\"]),\n",
    "            \"best_f1\": float(best[\"f1\"]),\n",
    "            \"approx_ap\": approx_ap,\n",
    "            \"top_by_f1\": pr_df.sort_values(\"f1\", ascending=False).head(12).to_dict(orient=\"records\"),\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "report_path = out_dir / \"report.json\"\n",
    "with report_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"saved:\", report_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
